{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Statements\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport PIL\nimport os\nimport torch\nimport torchvision\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nimport torchvision.models as models\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport csv\nimport random\nfrom transformers import AutoImageProcessor, ViTModel\nimport timm\n\n# if you are running this code in Jupyter notebook\n%matplotlib inline ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the seed\nSEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the Dataset:**","metadata":{}},{"cell_type":"code","source":"# Reading the labels \ndf = pd.read_csv('/kaggle/input/dog-breed-identification/labels.csv')\n\nnums = list(range(1, len(df)))\nrandom.shuffle(nums)\n\n# Splitting the Training, Validation and Test set\ntrain_df = df.loc[df.index.isin(nums[:int(len(df) * 0.7)])].reset_index().drop(columns = \"index\")\nval_df = df.loc[df.index.isin(nums[int(len(df) * 0.7):int(len(df) * 0.9)])].reset_index().drop(columns = \"index\")\ntest_df = df.loc[df.index.isin(nums[int(len(df) * 0.9):int(len(df) * 1)])].reset_index().drop(columns = \"index\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General dataset visualisation\ndf[\"breed\"].value_counts(ascending = True).plot(kind = \"barh\", fontsize = \"10\", title = \"Data Distribution\", figsize = (30,50))\n# plt.savefig(\"ClassDis1.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Custom class to load the Data\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, df, images_folder, transform = None):\n        self.df = df\n        self.images_folder = images_folder\n        self.transform = transform\n        \n        names= list(set(df['breed']))\n        num = [x for x in range(0,len(names))]\n        class2id = dict(zip(names,num))\n        self.class2index = class2id\n\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, index):\n        filename = self.df.loc[index][\"id\"]+'.jpg'\n        label = self.class2index[self.df.loc[index][\"breed\"]]\n        image = PIL.Image.open(os.path.join(self.images_folder, filename))\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard Transformation for the Validation and Test Set\ntransformation_val_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the Validation and Test set\nval_dataset = CustomDataset(val_df, \"/kaggle/input/dog-breed-identification/train\", transformation_val_test)\ntest_dataset = CustomDataset(test_df, \"/kaggle/input/dog-breed-identification/train\", transformation_val_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\n\nval_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True,  num_workers = 2)\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True,  num_workers = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training Loop:**","metadata":{}},{"cell_type":"code","source":"# Function for training the model on the Training set\ndef train(epochs, model, train_data, val_data, loss_fn, opt):\n\n    model.train()\n    train_loss = []\n    train_acc = []\n    val_loss = []\n    val_acc = []\n\n    for epoch in range(epochs):\n\n        epoch_run_loss = 0.0\n        epoch_run_acc = 0.0\n        epoch_run_loss_v = 0.0\n        epoch_run_acc_v = 0.0\n\n        for image, label in train_data:\n      \n            image = image.cuda()\n            label = label.cuda()\n            # Prediction of the model given the image\n            prediction = model(image)\n\n            # Getting the Loss\n            loss = loss_fn(prediction, label)\n\n            # Loss per epoch\n            epoch_run_loss = 0.9 * epoch_run_loss + 0.1 * loss.item()\n\n            # Accuracy per epoch \n            max_value, max_index = torch.max(prediction, dim = 1)\n            epoch_run_acc += (max_index == label).sum()\n\n            # Updating the weights\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n \n        # Training Loss and Accuracy\n        epoch_train_acc = (epoch_run_acc/len(train_data.dataset)).item()\n        train_loss.append(epoch_run_loss)\n        train_acc.append(epoch_train_acc)\n\n        print(\"Traning Loss at Epoch {}: {:.4f}\".format(epoch, epoch_run_loss))\n        print(\"Traning Accuracy at Epoch {}: {:.4f}\".format(epoch, epoch_train_acc))\n\n        # Validation Loss and Accuracy\n        model.eval()\n\n        for image, label in val_data:\n            image = image.cuda()\n            label = label.cuda()\n            # Prediction of the model given the image\n            prediction = model(image)\n\n            # Getting the Loss\n            loss = loss_fn(prediction, label)\n\n            # Loss per epoch\n            epoch_run_loss_v = 0.9 * epoch_run_loss_v + 0.1 * loss.item()\n\n\n            # Accuracy per epoch \n            max_value, max_index = torch.max(prediction, dim = 1)\n            epoch_run_acc_v += (max_index == label).sum()\n\n        epoch_val_acc = 100 * ((epoch_run_acc_v/len(val_data.dataset)).item())\n        val_loss.append(epoch_run_loss_v)\n        val_acc.append(epoch_val_acc)\n\n        print(\"Validation Loss at Epoch {}: {:.4f}\".format(epoch, epoch_run_loss_v))\n        print(\"Validation Accuracy at Epoch {}: {:.4f}\".format(epoch, epoch_val_acc))\n\n    return train_loss, train_acc, val_loss, val_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluation Loop:**","metadata":{}},{"cell_type":"code","source":"# Function for Evaluating the model\ndef test(model, test_data, loss_fn):\n\n    model.eval()\n\n    test_loss = 0.0\n    test_acc = 0.0\n\n    for image, label in test_data:\n\n        image = image.cuda()\n        label = label.cuda()\n        # Prediction of the model given the image\n        prediction = model(image)\n\n        # Getting the Loss\n        loss = loss_fn(prediction, label)\n\n        # Loss per epoch\n        test_loss += loss.item()\n\n        # Accuracy per epoch \n        max_value, max_index = torch.max(prediction, dim = 1)\n        test_acc += (max_index == label).sum()\n\n    test_loss = (test_loss / len(test_data.dataset))\n    test_acc = 100 * ((test_acc / len(test_data.dataset)).item())\n  \n    return test_loss, test_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualisation function:**","metadata":{}},{"cell_type":"code","source":"def plots(train_loss, validation_loss, train_accuracy, validation_accuracy):\n    epochs = []\n    \n    for i in range(10):\n        epochs.append(i)\n        \n    # Training and Validation Loss\n    plt.plot(epochs, train_loss, label = \"Training Loss\")\n    plt.plot(epochs, validation_loss, label = \"Validation Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(\"Loss.png\")\n    plt.show()\n    \n    for i in range(len(train_accuracy)):\n        train_accuracy[i] = train_accuracy[i] * 100\n        \n    # Training and Validation Accuracy\n    plt.plot(epochs, train_accuracy, label = \"Training Accuracy\")\n    plt.plot(epochs, validation_accuracy, label = \"Validation Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.savefig(\"Accuracy.png\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Transfomations and Augmentations:**","metadata":{}},{"cell_type":"code","source":"# Loading the Training set with the standard Transforms for Run 1\ntrain_dataset_run1 = CustomDataset(train_df, \"/kaggle/input/dog-breed-identification/train\", transformation_val_test)\ntrain_data_loader_run1 = torch.utils.data.DataLoader(train_dataset_run1, batch_size = BATCH_SIZE, shuffle = True,  num_workers = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformation and Augmentation for Run 2 - Random Rotation by a degree of 30\ntransformation_run2 = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomRotation(degrees = 30),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n])\n\n# Loading the Training set with the standard Transforms and rotation\ntrain_dataset_run2 = CustomDataset(train_df, \"/kaggle/input/dog-breed-identification/train\", transformation_run2)\ntrain_data_loader_run2 = torch.utils.data.DataLoader(train_dataset_run2, batch_size = BATCH_SIZE, shuffle = True,  num_workers = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformation and Augmentation for Run 3 - Colour Jitter\ntransformation_run3 = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ColorJitter(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n])\n\n# Loading the Training set with the standard Transforms and Colour Jitter\ntrain_dataset_run3 = CustomDataset(train_df, \"/kaggle/input/dog-breed-identification/train\", transformation_run3)\ntrain_data_loader_run3 = torch.utils.data.DataLoader(train_dataset_run3, batch_size = BATCH_SIZE, shuffle = True,  num_workers = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformation and Augmentation for Run 4 - Random Horizontal Flip\ntransformation_run4 = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n])\n\n# Loading the Training set with the standard Transforms and Horizontal Flip\ntrain_dataset_run4 = CustomDataset(train_df, \"/kaggle/input/dog-breed-identification/train\", transformation_run4)\ntrain_data_loader_run4 = torch.utils.data.DataLoader(train_dataset_run4, batch_size = BATCH_SIZE, shuffle = True,  num_workers = 2)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformation and Augmentation for Run 5 - Crop\ntransformation_run5 = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(size = 224),  \n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n])\n\n# Loading the Training set with the standard Transforms and Crop\ntrain_dataset_run5 = CustomDataset(train_df, \"/kaggle/input/dog-breed-identification/train\", transformation_run5)\ntrain_data_loader_run5 = torch.utils.data.DataLoader(train_dataset_run5, batch_size = BATCH_SIZE, shuffle = True,  num_workers = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformations and Augmentation for Run 6 - Rotation, Colour Jitter, Horizontal Flip and Crop\ntransformation_run6 = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomRotation(degrees = 30),\n    transforms.ColorJitter(),\n    transforms.RandomHorizontalFlip(),\n    transforms.CenterCrop(size = 224),  \n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])  \n])\n\n# Loading the Training set with all the Transforms \ntrain_dataset_run6 = CustomDataset(train_df, \"/kaggle/input/dog-breed-identification/train\", transformation_run6)\ntrain_data_loader_run6 = torch.utils.data.DataLoader(train_dataset_run6, batch_size = BATCH_SIZE, shuffle = True,  num_workers = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the loss function\nloss = nn.CrossEntropyLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. ResNet34:**","metadata":{}},{"cell_type":"code","source":"# Loading ResNet34 with the pre-trained weights\nresnet34 = models.resnet34(pretrained = True)\n\nfor params in resnet34.parameters():\n    params.require_grad = False\n\nresnet34.fc = nn.Linear(resnet34.fc.in_features, 120)\nresnet34.cuda()\n\noptimiser = optim.SGD(resnet34.parameters(), lr = 0.001, momentum = 0.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.1. ResNet34 - Standard Tranformations:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, resnet34, train_data_loader_run1, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(resnet34, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.2. ResNet34 - Rotation:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, resnet34, train_data_loader_run2, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(resnet34, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.3. ResNet34 - Colour Jitter:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, resnet34, train_data_loader_run3, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(resnet34, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.4. ResNet34 - Horizontal Flip:**","metadata":{}},{"cell_type":"code","source":"#Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(1, resnet34, train_data_loader_run4, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(resnet34, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.5. ResNet34 - Crop:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, resnet34, train_data_loader_run5, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(resnet34, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1.6. ResNet34 - Rotation, Colour Jitter, Horizontal Flip, Crop:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, resnet34, train_data_loader_run6, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(resnet34, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. ShuffleNet:**","metadata":{}},{"cell_type":"code","source":"# Loading ShuffleNet with the pre-trained weights\nshuffle = torchvision.models.shufflenet_v2_x1_0(pretrained = True)\n\nfor params in shuffle.parameters():\n    params.require_grad = False\n\nshuffle.fc = nn.Linear(1024, 120)\nshuffle.cuda()\n\n# Defining the optimiser as Stochastic Gradient Descent\noptimiser = optim.SGD(shuffle.parameters(), lr = 0.001, momentum = 0.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.1. ShuffleNet - Standard Tranformations:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, shuffle, train_data_loader_run1, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(shuffle, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.2. ShuffleNet - Rotation:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, shuffle, train_data_loader_run2, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(shuffle, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.3. ShuffleNet - Colour Jitter:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, shuffle, train_data_loader_run3, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(shuffle, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.4. ShuffleNet - Horizontal Flip:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, shuffle, train_data_loader_run4, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(shuffle, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.5. ShuffleNet - Crop:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, shuffle, train_data_loader_run5, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(shuffle, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2.6. ShuffleNet - Rotation, Colour Jitter, Horizontal Flip, Crop:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, shuffle, train_data_loader_run6, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(shuffle, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. VGG16:**","metadata":{}},{"cell_type":"code","source":"# Loading VGG16 with the pre-trained weights\nvgg_16 = models.vgg16(pretrained = True)\n\n# Changing the Last Fully Connected layer from 1000 classes to 120 classes\nvgg_16.classifier[6].out_features = 120\n\n# We will use the pre-trained weights for the Convolution layers\nfor param in vgg_16.features.parameters():\n    param.requires_grad = False\n\nvgg_16.cuda()\n\n# Defining the loss function\nloss = nn.CrossEntropyLoss()\n\n# Defining the optimiser as Stochastic Gradient Descent\noptimiser = optim.SGD(vgg_16.classifier.parameters(), lr = 0.0001, momentum = 0.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.1. VGG16 - Standard Tranformations:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vgg_16, train_data_loader_run1, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vgg_16, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.2. VGG16 - Rotation:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vgg_16, train_data_loader_run2, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vgg_16, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.3. VGG16 - Colour Jitter:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vgg_16, train_data_loader_run3, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vgg_16, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.4. VGG16 - Horizontal Flip:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vgg_16, train_data_loader_run4, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vgg_16, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.5. VGG16 - Crop:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vgg_16, train_data_loader_run5, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vgg_16, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **3.6. VGG16 - Rotation, Colour Jitter, Horizontal Flip, Crop:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vgg_16, train_data_loader_run6, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vgg_16, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Vision Transformers (ViT):**","metadata":{}},{"cell_type":"code","source":"class ViTBaseModel(nn.Module):\n    def __init__(self, num_classes, pretrained = False):\n\n        super(ViTBaseModel, self).__init__()\n\n        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained = True)\n\n        self.model.head = nn.Linear(self.model.head.in_features, num_classes)\n\n    def forward(self, input_images):\n        output = self.model(input_images)\n        return output\n\nvit_model = ViTBaseModel(num_classes = 120, pretrained = True)\nvit_model.cuda()\n\noptimiser = optim.SGD(vit_model.parameters(), lr = 0.001, momentum = 0.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.1. ViT - Standard Tranformations:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vit_model, train_data_loader_run1, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vit_model, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.2. ViT - Rotation:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vit_model, train_data_loader_run2, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vit_model, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.3. ViT - Colour Jitter:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vit_model, train_data_loader_run3, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vit_model, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.4. ViT - Horizontal Flip:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vit_model, train_data_loader_run4, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vit_model, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.5. ViT - Crop:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vit_model, train_data_loader_run5, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vit_model, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.6. ViT - Rotation, Colour Jitter, Horizontal Flip, Crop:**","metadata":{}},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, vit_model, train_data_loader_run6, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(vit_model, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Vision Transformer (ViT) (Not using a pre-trained model):**","metadata":{}},{"cell_type":"code","source":"def patch_image(images, patch_size):\n    # Image shape => (32, 3, 224, 224)\n    batch = images.shape[0]\n    channel = images.shape[1]\n    height = images.shape[2]\n    width = images.shape[3]\n    \n    # Each patch is of the size 16x16\n    patch_height = patch_size[0]\n    patch_width = patch_size[1]\n    # There are a total of 14x14 = 196 patches per image\n    num_patch_h = height // patch_height\n    num_patch_w = width // patch_width\n    \n    # Each image has 196 images, and the standard tranformer takes an embedding vector of 768\n    # This is the flattened output we are trying to get\n    # Output shape => (32, 196, 768)\n    batch_patch = torch.reshape(images, (batch, channel, num_patch_h, patch_height, num_patch_w, patch_width))\n    batch_patch = torch.permute(batch_patch, (0, 1, 2, 4, 3, 5))\n    batch_patch = torch.permute(batch_patch, (0, 2, 3, 4, 5, 1))\n    batch_patch = torch.reshape(images, (batch, num_patch_h * num_patch_w, channel * patch_height * patch_width))\n    \n    return batch_patch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ViT(nn.Module):\n    \n    def __init__(self, img_size, patch_size, emb_dim, num_heads, dim_feedforward, blocks, num_classes):\n        super(ViT, self).__init__()\n    \n        # img_size = image size with (channels, height, width)\n        # patch_size = Dimensions of each patch in an image\n        # emd_dim = Linear tranformation value\n        # num_heads = number of heads for the multi head attention models\n        # dim_feedforward = Dimensions of the feed forward network (2048 is default - pytorch)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        n_p = (img_size[1] // patch_size[0]) * (img_size[2] // patch_size[1])\n        \n        token_size = patch_size[0] * patch_size[1] * img_size[0]\n        self.linear = nn.Linear(token_size, emb_dim)\n        \n        # Classification token with the shape => (1, 1, 196)\n        self.c_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n        \n        # Positional Emdedding\n        self.pos_emb = nn.Parameter(torch.randn(n_p, emb_dim))\n        \n        # Transfomer model in pytorch\n        encoder_layer = nn.TransformerEncoderLayer(emb_dim, num_heads, dim_feedforward, activation = \"gelu\", batch_first = True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, blocks)\n        \n        self.mlp = nn.Sequential(nn.Linear(emb_dim, num_classes), nn.Softmax(dim = -1))\n    \n    def forward(self, images):\n        \n        batch = images.shape[0]\n        channel = images.shape[1]\n        height = images.shape[2]\n        width = images.shape[3]\n        \n        # patch shape => (32, 196, 768)\n        patches = patch_image(images, self.patch_size)\n        \n        # Linear Layer, shape => (32, 196, 196)\n        tokens = self.linear(patches)\n        \n        # Adding the Classification token \n        # Shape => (32, 1, 196)\n        c_token = self.c_token.expand([batch, -1, -1])\n        \n        # Positional Embedding shape => (32, 196, 196)\n        embedding = tokens + self.pos_emb\n        \n        # Concatenate the classification token with other embedding\n        # Shape => (32, 197, 196)\n        embedding = torch.cat([c_token, tokens], axis = 1)\n        \n        # Passing the embedding to the Transformer\n        # transformer gives the same shape as the input\n        # shape => (32, 197, 196)\n        transformer = self.transformer(embedding)\n        \n        # Classification head which outputs the probability of each class\n        # This is only done with the classification token at the top of the embedding\n        # classification_token, shape => (32, 1, 196)\n        classification_token = transformer[:,0,:]\n        \n        # Shape => (32, 120)\n        classification = self.mlp(classification_token)\n        result = torch.argmax(classification, dim = 1)\n        \n        return classification","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ViT(\n    img_size=(3, 224, 224),\n    patch_size = (16, 16),\n    emb_dim = 196, \n    num_heads = 4, \n    dim_feedforward = 2048, \n    blocks = 8, \n    num_classes = 120\n  )\nmodel.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\ntrain_loss, train_accuracy, validation_loss, validation_accuracy = train(10, model, train_data_loader_run4, val_data_loader, loss, optimiser)\n\n# Visualisation\nplots(train_loss, validation_loss, train_accuracy, validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(model, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **6. CNN:**","metadata":{}},{"cell_type":"code","source":"class BaselineCNN(nn.Module):\n    def __init__(self):\n        super(BaselineCNN, self).__init__()\n\n        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.conv_layer3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.fc1 = nn.Linear(in_features=128 * 28 * 28, out_features=500)\n        self.fc2 = nn.Linear(in_features=500, out_features=120)\n\n        self.dropout = nn.Dropout(p=0.3)\n        \n        \n    def forward(self, input_tensor):\n\n        x = self.maxpool(F.relu(self.conv_layer1(input_tensor)))\n        x = self.maxpool(F.relu(self.conv_layer2(x)))\n        x = self.maxpool(F.relu(self.conv_layer3(x)))\n        \n        # Flatten \n        x = x.view(-1, 128 * 28 * 28)\n        \n\n        x = self.dropout(x)\n        \n\n        x = F.relu(self.fc1(x))\n    \n        x = self.dropout(x)\n        \n        x = self.fc2(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiate the CNN\nmodel_scratch = BaselineCNN()\nmodel_scratch.cuda()\n\noptimiser = torch.optim.SGD(model_scratch.parameters(), lr = 0.01, momentum = 0.9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_train_loss, base_train_accuracy, base_validation_loss, base_validation_accuracy = train(10, model_scratch, train_data_loader_run4, val_data_loader, loss, optimiser)\nplots(base_train_loss, base_validation_loss, base_train_accuracy, base_validation_accuracy)\n\n# Accuracy and Loss on the Test set\ntest_loss, test_accuracy = test(model_scratch, test_data_loader, loss)\nprint(\"Test Loss = \", test_loss)\nprint(\"Test Accuracy = \", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}